{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import csv\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Cleanup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \"\"\"Split CSV file into training and testing\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(dataset)\n",
    "    msk = np.random.rand(len(df)) <= 0.7\n",
    "    \n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    train.to_csv(f\"{dataset}_train\", index=False)\n",
    "    test.to_csv(f\"{dataset}_test\", index=False)\n",
    "\n",
    "split_dataset(\"spec2006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_microbenchmarks(path, name, phase):\n",
    "    \"\"\"Combine individual benchmark files from microbenchmarks\"\"\"\n",
    "    \n",
    "    discard_files = [\"clock.txt\", \"time.txt\", \".DS_Store\"]\n",
    "    configs = {'LITTLE': \"0\", 'big': \"1\"}\n",
    "    config_folders = os.listdir(path)\n",
    "    config_folders = [folder for folder in config_folders if folder not in discard_files]\n",
    "    dataframe_collections = []\n",
    "    for config in config_folders:\n",
    "        \n",
    "        # Ensure path is a folder\n",
    "        if not os.path.isdir(f\"{path}/{config}/\"):\n",
    "            continue\n",
    "            \n",
    "        # Skip big config (for testing purposes)\n",
    "        if config == \"big\":\n",
    "            continue\n",
    "\n",
    "        # List the files, each containing date points for a PMC\n",
    "        data_files = os.listdir(f\"{path}/{config}/\")\n",
    "        data_files = [file for file in data_files if file not in discard_files and file.endswith(\".txt\")]\n",
    "        \n",
    "        # Append column headers to CSV\n",
    "        column_headers = [file.split('.')[0] for file in data_files]\n",
    "        column_headers.append(\"cluster\")\n",
    "        column_headers.append(\"phase\")\n",
    "        \n",
    "        # Insert data from individual file as a column\n",
    "        df = pd.DataFrame(columns=column_headers)\n",
    "        for file in data_files:\n",
    "            with open(f\"{path}/{config}/{file}\", \"r\") as text_file:\n",
    "                lines = text_file.read().splitlines()\n",
    "                df[file.split('.')[0]] = lines\n",
    "                df['cluster'] = configs[config]\n",
    "                df['phase'] = phase\n",
    "        df = df.iloc[100:] # skip first 'x' rows (first few values may not be representative of the phase)\n",
    "        df = df.iloc[:-100] # skip last 'x' rows (last few values may not be representative of the phase)\n",
    "        dataframe_collections.append(df)\n",
    "        \n",
    "        # Create and return CSV\n",
    "        df.to_csv(f\"{path}/{config}/{name}_{config}.csv\", index=False)\n",
    "        master_dataframes = pd.concat(dataframe_collections)\n",
    "        master_dataframes.to_csv(f\"{path}/{name}.csv\", index=False)\n",
    "\n",
    "    return master_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_spec2006(path, name, phase):\n",
    "    \"\"\"Combine individual benchmark files for SPEC2006\"\"\"\n",
    "    \n",
    "    # Ensure path is valid\n",
    "    if not os.path.isdir(path):\n",
    "        return\n",
    "\n",
    "    discard_files = [\"dtlb_misses.txt\", \"l1_data_accesses.txt\", \"clock.txt\", \"time.txt\", \".DS_Store\"]\n",
    "    configs = {'LITTLE': \"0\", 'big': \"1\"}\n",
    "    config_folders = os.listdir(path)\n",
    "    config_folders = [folder for folder in config_folders if folder not in discard_files]\n",
    "    dataframe_collections = []\n",
    "    for config in config_folders:\n",
    "        \n",
    "        # Ensure path is a folder\n",
    "        if not os.path.isdir(f\"{path}/{config}/\"):\n",
    "            continue\n",
    "            \n",
    "        # Skip big config (for testing purposes)\n",
    "        if \"big\" in config:\n",
    "            continue\n",
    "\n",
    "        # List the files, each containing date points for a PMC\n",
    "        data_files = os.listdir(f\"{path}/{config}/\")\n",
    "        data_files = [file for file in data_files if file not in discard_files and file.endswith(\".txt\")]\n",
    "\n",
    "        # Append column headers to CSV\n",
    "        column_headers = [file.split('.')[0] for file in data_files]\n",
    "        column_headers.append(\"cluster\")\n",
    "        column_headers.append(\"phase\")\n",
    "\n",
    "        # Insert data from individual file as a column\n",
    "        df = pd.DataFrame(columns=column_headers)\n",
    "        for file in data_files:\n",
    "            with open(f\"{path}/{config}/{file}\", \"r\") as text_file:\n",
    "                lines = text_file.read().splitlines()\n",
    "                df[file.split('.')[0]] = lines\n",
    "                df['cluster'] = \"0\" if \"LITTLE\" in config else \"1\"\n",
    "                df['phase'] = phase\n",
    "        df = df.iloc[200:] # skip first 'x' rows (first few values may not be representative of the phase)\n",
    "        df = df.iloc[:-200] # skip last 'x' rows (last few values may not be representative of the phase)\n",
    "        dataframe_collections.append(df)\n",
    "        \n",
    "        # Create and return CSV\n",
    "        df.to_csv(f\"{path}/{config}/{name}_{config}.csv\", index=False)\n",
    "        master_dataframes = pd.concat(dataframe_collections)\n",
    "        master_dataframes.to_csv(f\"{path}/{name}.csv\", index=False)\n",
    "\n",
    "    return master_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets_microbenchmarks():\n",
    "    \"\"\"Generate CSVs from raw PMC files from microbenchmarks\"\"\"\n",
    "    \n",
    "    path = \"raw-data/phases\"\n",
    "    phase = {'cpu': \"0\", 'mem': \"1\"}\n",
    "    benchmarks = os.listdir(path)\n",
    "    dataframe_collections = []\n",
    "    for benchmark in benchmarks:\n",
    "        if not os.path.isdir(f\"{path}/{benchmark}\"):\n",
    "            continue\n",
    "                                        \n",
    "        phase_folders = os.listdir(f\"{path}/{benchmark}\")\n",
    "        for folder in phase_folders:\n",
    "            folder_path = f\"{path}/{benchmark}/{folder}\"\n",
    "            if folder in phase:\n",
    "                df = combine_microbenchmarks(folder_path, folder, phase[folder])\n",
    "                dataframe_collections.append(df)\n",
    "\n",
    "        master_dataframes = pd.concat(dataframe_collections)\n",
    "        master_dataframes.to_csv('datasets/micro_dataset.csv', index=False)\n",
    "\n",
    "generate_datasets_microbenchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets_spec2006():\n",
    "    \"\"\"Generate CSVs from raw PMC files for SPEC2006\"\"\"\n",
    "    \n",
    "    # '2': phased, '1': memory-intensive, '0': cpu-intensive\n",
    "    phases = {\n",
    "        'astar': \"2\",\n",
    "        'sjeng': \"2\",\n",
    "        'leslie3d': \"2\",\n",
    "        'GemsFDTD': \"1\",\n",
    "        'mcf':  \"1\",\n",
    "        'milc':  \"1\",\n",
    "        'calculix':  \"0\",\n",
    "        'gromacs': \"0\",\n",
    "        'povray': \"0\",\n",
    "    }\n",
    "    path = \"raw-data/mosse-spec2006\"\n",
    "    discard_files = [\".DS_Store\"]\n",
    "    benchmarks = os.listdir(path)\n",
    "    benchmarks = [b for b in benchmarks if b not in discard_files]\n",
    "    cpu_benchmarks = [\"povray\", \"gromacs\", \"calculix\"]\n",
    "    mem_benchmarks = [\"GemsFDTD\", \"mcf\", \"milc\"]\n",
    "    # phased_benchmarks = [\"astar\", \"sjeng\", \"leslie3d\"]\n",
    "    benchmarks = [val for pair in zip(cpu_benchmarks, mem_benchmarks) for val in pair]\n",
    "    # benchmarks += phased_benchmarks\n",
    "    dataframe_collections = []\n",
    "    for benchmark in benchmarks:\n",
    "        # Combine SPEC2006 benchmarks into a master CSV\n",
    "        configuration_folders = os.listdir(f\"{path}/{benchmark}\")\n",
    "        folder_path = f\"{path}/{benchmark}\"\n",
    "        print(benchmark) # debug\n",
    "        df = combine_spec2006(folder_path, benchmark, phases[benchmark])\n",
    "        \n",
    "        # Append to master dataframe\n",
    "        dataframe_collections.append(df)\n",
    "        master_dataframes = pd.concat(dataframe_collections)\n",
    "        master_dataframes.to_csv('datasets/spec2006_dataset.csv', index=False)\n",
    "\n",
    "generate_datasets_spec2006()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(datasets):\n",
    "    \"\"\"Combine multiple datasets\"\"\"\n",
    "    \n",
    "    print(datasets)\n",
    "    master = pd.concat([pd.read_csv(dataset) for dataset in datasets])      \n",
    "    master.to_csv(\"training_master_dataset.csv\", index=False)\n",
    "    \n",
    "datasets = [\"smoothed_spec2006.csv\", \"smoothed_micro.csv\"]\n",
    "combine_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Plot Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def filter_data(col):\n",
    "    \"\"\"Helper function to perform Savitzky–Golay filter\"\"\"\n",
    "    return savgol_filter(col, 53, 1).astype(int)\n",
    "\n",
    "def filter_benchmarks(dataset_name):\n",
    "    \"\"\"Filter datasets using Savitzky–Golay filter\"\"\"\n",
    "    \n",
    "    # Read dataset\n",
    "    df = pd.read_csv(f\"datasets/{dataset_name}_dataset.csv\")\n",
    "    smooth_cols = df.iloc[:,:-2]\n",
    "    unsmooth_cols = df.iloc[:,-2:]\n",
    "    \n",
    "    # Filter the dataset using Savitzky-Golay smoothing filter\n",
    "    smoothed_data = smooth_cols.apply(filter_data)\n",
    "    smoothed_data = pd.concat([smoothed_data, unsmooth_cols], axis=1)\n",
    "    smoothed_data.to_csv(f\"datasets/smoothed_{dataset_name}.csv\", index=False)\n",
    "    \n",
    "filter_benchmarks(\"spec2006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset):\n",
    "    \"\"\"Generate and filter dataset\"\"\"\n",
    "    \n",
    "    if dataset == \"spec\":\n",
    "        generate_datasets_spec2006()\n",
    "        filter_benchmarks(\"spec2006\")\n",
    "    else:\n",
    "        generate_datasets_microbenchmarks()\n",
    "        filter_benchmarks(\"micro\")\n",
    "\n",
    "create_datasets(\"spec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_datasets(dataset):\n",
    "    \"\"\"Plot datasets\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(f\"datasets/{dataset}\")\n",
    "    plt.plot(df['instr'])\n",
    "    \n",
    "plot_datasets(\"smoothed_spec2006.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_groundtruth(dataset):\n",
    "    \"\"\"Calculate ground-truth for applications\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(dataset)\n",
    "    \n",
    "    master = pd.DataFrame()\n",
    "    master['bus/instr'] = df['bus_accesses'] / df['instr']\n",
    "    plt.plot(master['bus/instr'])\n",
    "    \n",
    "calculate_groundtruth(\"smoothed_spec2006.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference(dataset):\n",
    "    \"\"\"Calculate difference between points\"\"\"\n",
    "    \n",
    "    # Read dataset\n",
    "    dataset = pd.read_csv(dataset)\n",
    "    discard = [\"phase\", \"cluster\"]\n",
    "    pmcs = [pmc for pmc in dataset if pmc not in discard]\n",
    "\n",
    "    # Calculate percentage difference\n",
    "    master = pd.DataFrame()\n",
    "    alpha = 0.9\n",
    "    window_size = 100\n",
    "    \n",
    "    for pmc in dataset:\n",
    "        if pmc == \"cluster\" or pmc == \"phase\":\n",
    "            continue\n",
    "        col = dataset[pmc]\n",
    "        col_size = len(col)\n",
    "        pct_change = []\n",
    "        \n",
    "        for i in range(window_size, col_size):\n",
    "            prev_avg = ((sum(col[i - window_size:i]) / window_size) * (1-alpha))\n",
    "            curr = (col[i] * alpha)\n",
    "            \n",
    "            # calculate percentage change\n",
    "            change = (curr - prev_avg) / (prev_avg)\n",
    "            pct_change.append(change)\n",
    "        master[pmc] = pct_change\n",
    "    \n",
    "    for d in discard:\n",
    "        master[d] = dataset[d]\n",
    "    \n",
    "    # Determine phase changes\n",
    "    col = dataset['phase']\n",
    "    col_size = len(dataset['phase'])\n",
    "    phase_changes = []\n",
    "    for i in range(window_size, col_size):\n",
    "        col_window = col[i - window_size:i]\n",
    "        phase_change = len(set(col_window)) > 1\n",
    "        if phase_change == True:\n",
    "            phase_changes.append(\"1\")\n",
    "        else:\n",
    "            phase_changes.append(\"0\")\n",
    "            \n",
    "    master['change'] = phase_changes\n",
    "    \n",
    "    # Write dataframe to CSV \n",
    "    master.to_csv('datasets/spec2006_pctchange.csv', index=False)\n",
    "    \n",
    "calculate_difference(\"datasets/smoothed_spec2006.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "def train_model():\n",
    "    # Read dataset\n",
    "    dataset = pd.read_csv(\"test2.csv\")\n",
    "    labels = dataset.iloc[:,-1:]\n",
    "    features = dataset[['cycles', 'instr', 'llc_accesses', 'llc_misses', 'dtlb_misses']]\n",
    "\n",
    "    # Split dataset into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels,\n",
    "                                                        test_size=0.15, shuffle=True)\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train SVM model\n",
    "    \"\"\"\n",
    "    model = SVC(kernel=\"rbf\")\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    \"\"\"\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Test SVM model\n",
    "    test_score = model.score(X_test, y_test.values.ravel())\n",
    "    print(test_score)\n",
    "\n",
    "    # Output model to pickle file\n",
    "    pickle.dump(model, open(\"phases-model.pkl\", \"wb\"))\n",
    "\n",
    "    # Output scalr to pickle file\n",
    "    pickle.dump(scaler, open(\"phases-scaler.pkl\", \"wb\"))\n",
    "    \n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pickle import load\n",
    "\n",
    "# Read dataset\n",
    "dataset = pd.read_csv(\"test1.csv\")\n",
    "labels = dataset.iloc[:,-1:]\n",
    "# features = dataset.iloc[:,:-1]\n",
    "features = dataset[['cycles', 'instr', 'llc_accesses', 'llc_misses', 'bus_accesses']]\n",
    "\n",
    "# Load models \n",
    "model = load(open(\"phases-model.pkl\", \"rb\"))\n",
    "scaler = load(open(\"phases-scaler.pkl\", \"rb\"))\n",
    "\n",
    "# Scale data\n",
    "features = scaler.transform(features)\n",
    "\n",
    "# Test SVM model\n",
    "test_score = model.score(features, labels)\n",
    "print(test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
